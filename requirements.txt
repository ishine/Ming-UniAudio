torch==2.6.0
torchaudio==2.6.0
torchvision==0.21.0
tokenizers
grouped_gemm==0.1.4
# funasr==1.1.14
peft==0.17.1
diffusers==0.33.0
decord==0.6.0
hyperpyyaml==1.2.2
soundfile==0.12.1
transformers==4.52.4
# flash_attn==2.7.0  # to accelerate installation process, please download wheel package by `wget https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.0.post1/flash_attn-2.7.0.post1+cu12torch2.4cxx11abiFALSE-cp310-cp310-linux_x86_64.whl`
x_transformers
torchdiffeq
torchtune
torchao
ipynbname
accelerate==1.3.0
