{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4770bf52-16aa-486e-9a5e-dad64a8ad658",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2025-10-01T13:46:36.742745Z",
     "shell.execute_reply.started": "2025-10-01T13:46:36.738714Z",
     "to_execute": "2025-10-01T13:46:36.673Z"
    },
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt3/qiuzhihao.qzh/Ming-Lite-UniAudio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /path/to/workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28108cc-d941-4910-ac00-6106e68c2d99",
   "metadata": {
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00bfc4a-38b2-4afa-899b-1ea0c6483c7b",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2025-10-01T13:47:13.832616Z",
     "shell.execute_reply.started": "2025-10-01T13:46:45.430862Z",
     "to_execute": "2025-10-01T13:46:45.379Z"
    },
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[2025-10-01 21:46:50,270] [INFO] [add_hooks.py:71:__init__] If you are working on AIStudio, please set 'ANTMONITOR_TFEVENT_PATH' env.\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:02<00:00,  3.82it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import torch\n",
    "from transformers import AutoProcessor\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import display\n",
    "import ipynbname\n",
    "notebook_path = ipynbname.path()\n",
    "current_dir = os.path.dirname(notebook_path)\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from modeling_bailingmm import BailingMMNativeForConditionalGeneration\n",
    "import random\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "from sentence_manager.sentence_manager import SentenceNormalizer\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "def seed_everything(seed=1895):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class MingAudio:\n",
    "    def __init__(self, model_path, device=\"cuda:0\"):\n",
    "        self.device = device\n",
    "        self.model = BailingMMNativeForConditionalGeneration.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            low_cpu_mem_usage=True,\n",
    "        ).eval().to(torch.bfloat16).to(self.device)\n",
    "        self.processor = AutoProcessor.from_pretrained(\".\", trust_remote_code=True)\n",
    "        self.tokenizer = self.processor.tokenizer\n",
    "        self.sample_rate = self.processor.audio_processor.sample_rate\n",
    "        self.patch_size = self.processor.audio_processor.patch_size\n",
    "        self.normalizer = self.init_tn_normalizer(tokenizer=self.tokenizer)\n",
    "\n",
    "    def init_tn_normalizer(self, config_file_path=None, tokenizer=None):\n",
    "\n",
    "        if config_file_path is None:\n",
    "            default_config_path = os.path.join(\n",
    "                os.path.dirname(os.path.dirname(os.path.realpath(__file__))), \n",
    "                \"sentence_manager/default_config.yaml\"\n",
    "            )\n",
    "            config_file_path = default_config_path\n",
    "        with open(config_file_path, 'r') as f:\n",
    "            self.sentence_manager_config = yaml.safe_load(f)\n",
    "        if \"split_token\" not in self.sentence_manager_config:\n",
    "            self.sentence_manager_config[\"split_token\"] = []\n",
    "        assert isinstance(self.sentence_manager_config[\"split_token\"], list)\n",
    "        if tokenizer is not None:\n",
    "            self.sentence_manager_config[\"split_token\"].append(re.escape(tokenizer.eos_token))\n",
    "        normalizer = SentenceNormalizer(self.sentence_manager_config.get(\"text_norm\", {}))\n",
    "        \n",
    "        return normalizer\n",
    "\n",
    "    def speech_understanding(self, messages, lang=None):\n",
    "        text = self.processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        image_inputs, video_inputs, audio_inputs = self.processor.process_vision_info(messages)\n",
    "\n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            audios=audio_inputs,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.device)\n",
    "        \n",
    "        if lang is not None:\n",
    "            language = torch.tensor([self.tokenizer.encode(f'{lang}\\t')]).to(inputs['input_ids'].device)\n",
    "            inputs['input_ids'] = torch.cat([inputs['input_ids'], language], dim=1)\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            inputs['attention_mask'] = torch.ones(inputs['input_ids'].shape, dtype=attention_mask.dtype)\n",
    "        for k in inputs.keys():\n",
    "            if k == \"pixel_values\" or k == \"pixel_values_videos\" or k == \"audio_feats\":\n",
    "                inputs[k] = inputs[k].to(dtype=torch.bfloat16)\n",
    "        logger.info(f\"input: {self.tokenizer.decode(inputs['input_ids'].cpu().numpy().tolist()[0])}\")\n",
    "\n",
    "        generated_ids = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            eos_token_id=self.processor.gen_terminator,\n",
    "        )\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        output_text = self.processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )[0]\n",
    "\n",
    "        return output_text\n",
    "\n",
    "    def speech_generation(\n",
    "        self, \n",
    "        text,\n",
    "        prompt_wav_path,\n",
    "        prompt_text,\n",
    "        lang='zh',\n",
    "        output_wav_path='out.wav'\n",
    "    ):\n",
    "        text = self.normalizer.normalize(text)\n",
    "        waveform = self.model.generate_tts(\n",
    "            text=text,\n",
    "            prompt_wav_path=prompt_wav_path,\n",
    "            prompt_text=prompt_text,\n",
    "            patch_size=self.patch_size,\n",
    "            tokenizer=self.tokenizer,\n",
    "            lang=lang,\n",
    "            output_wav_path=output_wav_path,\n",
    "            sample_rate=self.sample_rate,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        return waveform\n",
    "\n",
    "    def speech_edit(\n",
    "        self, \n",
    "        messages,\n",
    "        output_wav_path='out.wav',\n",
    "        use_cot=True\n",
    "    ):\n",
    "        text = self.processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        image_inputs, video_inputs, audio_inputs = self.processor.process_vision_info(messages)\n",
    "\n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            audios=audio_inputs,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.device)\n",
    "\n",
    "        if use_cot:\n",
    "            ans = torch.tensor([self.tokenizer.encode('<answer>')]).to(inputs['input_ids'].device)\n",
    "            inputs['input_ids'] = torch.cat([inputs['input_ids'], ans], dim=1)\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            inputs['attention_mask'] = torch.ones(inputs['input_ids'].shape, dtype=attention_mask.dtype)\n",
    "        for k in inputs.keys():\n",
    "            if k == \"pixel_values\" or k == \"pixel_values_videos\" or k == \"audio_feats\":\n",
    "                inputs[k] = inputs[k].to(dtype=torch.bfloat16)\n",
    "        logger.info(f\"input: {self.tokenizer.decode(inputs['input_ids'].cpu().numpy().tolist()[0])}\")\n",
    "\n",
    "        edited_speech, edited_text = self.model.generate_edit(\n",
    "            **inputs,\n",
    "            tokenizer=self.tokenizer,\n",
    "            output_wav_path=output_wav_path\n",
    "        )\n",
    "        return edited_speech, edited_text\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # model = MingAudio(\"/path/to/model\")\n",
    "    # load base model\n",
    "    model = MingAudio(\"inclusionAI/Ming-UniAudio-16B-A3B\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8c3e1e-690a-4232-94b6-3d0ce26879fa",
   "metadata": {
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170c71f9-336a-433a-9e4a-a6980b938e4d",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2025-09-30T12:54:28.288569Z",
     "shell.execute_reply.started": "2025-09-30T12:54:26.623333Z",
     "to_execute": "2025-09-30T12:54:26.556Z"
    },
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 20:54:26.673\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_understanding\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1minput: <role>HUMAN</role>Please recognize the language of this speech and transcribe it. Format: oral.<audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><role>ASSISTANT</role>\u001b[0m\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "2025-09-30 20:54:27,589 - modeling_bailing_moe - WARNING - The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "\u001b[32m2025-09-30 20:54:28.285\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mGenerated Response: Chinese\t现在是不是也该长点心了吧\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # ASR\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral.\",\n",
    "                },\n",
    "                \n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/BAC009S0915W0292.wav\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_understanding(messages=messages)\n",
    "    logger.info(f\"Generated Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2de9ed2d-0eb5-4720-b9bc-4e5555ff8827",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2025-10-01T13:47:31.844488Z",
     "shell.execute_reply.started": "2025-10-01T13:47:27.411064Z",
     "to_execute": "2025-10-01T13:47:27.344Z"
    },
    "isLargeOutputDisplay": false,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-01 21:47:27.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_understanding\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1minput: <role>HUMAN</role>Please recognize the language of this speech and transcribe it. Format: oral.<audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><role>ASSISTANT</role>川渝\t\u001b[0m\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "2025-10-01 21:47:28,317 - modeling_bailing_moe - WARNING - The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "\u001b[32m2025-10-01 21:47:28.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mGenerated Response: 我难受得很别个都睡了\u001b[0m\n",
      "\u001b[32m2025-10-01 21:47:28.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_understanding\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1minput: <role>HUMAN</role>Please recognize the language of this speech and transcribe it. Format: oral.<audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><role>ASSISTANT</role>湖南\t\u001b[0m\n",
      "\u001b[32m2025-10-01 21:47:29.605\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mGenerated Response: 我们到你屋里大概一点半左右\u001b[0m\n",
      "\u001b[32m2025-10-01 21:47:29.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_understanding\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1minput: <role>HUMAN</role>Please recognize the language of this speech and transcribe it. Format: oral.<audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><role>ASSISTANT</role>闽南\t\u001b[0m\n",
      "\u001b[32m2025-10-01 21:47:30.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mGenerated Response: 宝贝较早休困晚安\u001b[0m\n",
      "\u001b[32m2025-10-01 21:47:30.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_understanding\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1minput: <role>HUMAN</role>Please recognize the language of this speech and transcribe it. Format: oral.<audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><role>ASSISTANT</role>上海\t\u001b[0m\n",
      "\u001b[32m2025-10-01 21:47:30.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mGenerated Response: 阿拉考试还没定下来唻\u001b[0m\n",
      "\u001b[32m2025-10-01 21:47:30.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_understanding\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1minput: <role>HUMAN</role>Please recognize the language of this speech and transcribe it. Format: oral.<audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><role>ASSISTANT</role>Canton\t\u001b[0m\n",
      "\u001b[32m2025-10-01 21:47:31.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mGenerated Response: 你做乜嘢啊系咪唔想倾偈啊\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Dialect ASR\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral.\",\n",
    "                },\n",
    "                \n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/chuanyu_demo.wav\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_understanding(messages=messages, lang=\"川渝\")\n",
    "    logger.info(f\"Generated Response: {response}\")\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral.\",\n",
    "                },\n",
    "                \n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/hunan_demo.wav\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_understanding(messages=messages, lang=\"湖南\")\n",
    "    logger.info(f\"Generated Response: {response}\")\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral.\",\n",
    "                },\n",
    "                \n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/minnan_demo.wav\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_understanding(messages=messages, lang=\"闽南\")\n",
    "    logger.info(f\"Generated Response: {response}\")\n",
    "\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral.\",\n",
    "                },\n",
    "                \n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/shanghai_demo.wav\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_understanding(messages=messages, lang=\"上海\")\n",
    "    logger.info(f\"Generated Response: {response}\")\n",
    "\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral.\",\n",
    "                },\n",
    "                \n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/yueyu_demo.wav\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_understanding(messages=messages, lang=\"Canton\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41cc7088-a1ec-4e17-84d3-6047a30a6557",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2025-09-30T12:56:01.997703Z",
     "shell.execute_reply.started": "2025-09-30T12:55:48.795689Z",
     "to_execute": "2025-09-30T12:55:48.789Z"
    },
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating zh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<role>HUMAN</role>Please translate the text to speech.\n",
      "在此奉劝大家别乱打美白针。我们的愿景是构建未来服务业的数字化基础设施，为世界带来更多微小而美好的改变。<role>ASSISTANT</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 96/300 [00:13<00:27,  7.37it/s]\n",
      "\u001b[32m2025-09-30 20:56:01.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mwaveform: tensor([[ 5.7267e-04,  1.2557e-03,  8.2468e-04,  ..., -2.2724e-06,\n",
      "         -1.6844e-06, -2.2619e-05]])\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StopInfo: 96 299\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # TTS\n",
    "    waveform = model.speech_generation(\n",
    "        text='我们的愿景是构建未来服务业的数字化基础设施，为世界带来更多微小而美好的改变。',\n",
    "        prompt_wav_path='data/wavs/10002287-00000094.wav',\n",
    "        prompt_text='在此奉劝大家别乱打美白针。',\n",
    "        output_wav_path='data/output/tts.wav',\n",
    "    )\n",
    "    logger.info(f\"waveform: {waveform}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dcaf98-133e-4f77-83e6-4b9c7b1eb096",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2025-09-30T12:57:10.168924Z",
     "shell.execute_reply.started": "2025-09-30T12:57:03.240371Z",
     "to_execute": "2025-09-30T12:57:03.392Z"
    },
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 20:57:03.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And insert '实现' before the character or word at index 3.\n",
      "</prompt><role>ASSISTANT</role><answer>\u001b[0m\n",
      "\u001b[32m2025-09-30 20:57:10.166\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[-0.0006, -0.0005, -0.0001,  ...,  0.0016,  0.0013,  0.0012]]],\n",
      "       device='cuda:0'), '<cot_text>有望<edit><framePatch></edit>盘活超万亿的国企存量资产</cot_text><gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "    # load sft model\n",
    "    model = MingAudio(\"inclusionAI/Ming-UniAudio-16B-A3B-Edit\")\n",
    "    # Ins\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/00004768-00000024.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And insert '实现' before the character or word at index 3.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_edit(messages=messages, output_wav_path=\"data/output/ins.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2813319d-6664-4acd-b5b2-62dedeb905c0",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2025-09-30T13:01:17.727209Z",
     "shell.execute_reply.started": "2025-09-30T13:01:12.996745Z",
     "to_execute": "2025-09-30T13:01:13.002Z"
    },
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:01:13.133\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And delete the characters or words from index 5 to index 8.\n",
      "</prompt><role>ASSISTANT</role><answer>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:01:17.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[-0.0020, -0.0023, -0.0018,  ..., -0.0025, -0.0023, -0.0020]]],\n",
      "       device='cuda:0'), '<cot_text>有望盘活<edit></edit>国企存量资产</cot_text><gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Del\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/00004768-00000024.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And delete the characters or words from index 5 to index 8.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_edit(messages=messages, output_wav_path=\"data/output/del.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81350375-59fd-47e6-a145-bd8abcc397c8",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2025-09-30T13:02:55.737318Z",
     "shell.execute_reply.started": "2025-09-30T13:02:49.667764Z",
     "to_execute": "2025-09-30T13:02:49.650Z"
    },
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:02:49.760\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And substitute '盘活' with '创造'.\n",
      "</prompt><role>ASSISTANT</role><answer>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:02:55.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[-0.0007, -0.0006, -0.0002,  ...,  0.0024,  0.0020,  0.0019]]],\n",
      "       device='cuda:0'), '<cot_text>有望<edit><framePatch></edit>超万亿的国企存量资产</cot_text><gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Sub\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/00004768-00000024.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And substitute '盘活' with '创造'.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_edit(messages=messages, output_wav_path=\"data/output/sub.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70e50ebb-6600-46fe-8135-1bb58b202c81",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2025-09-30T13:05:22.754867Z",
     "shell.execute_reply.started": "2025-09-30T13:05:08.896957Z",
     "to_execute": "2025-09-30T13:05:08.858Z"
    },
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:05:08.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And denoise the audio.\n",
      "</prompt><role>ASSISTANT</role>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:05:22.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[-8.8062e-03,  1.3475e-02,  5.8888e-03,  ...,  2.6246e-06,\n",
      "          -2.5213e-06,  4.4239e-07]]], device='cuda:0'), '<gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Denoise\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/noreverb_fileid_0.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And denoise the audio.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_edit(messages=messages, use_cot=False, output_wav_path=\"data/output/denoise.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ed9e33c-55ea-48cc-b03e-2ab83ed28225",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2025-09-30T13:07:17.858355Z",
     "shell.execute_reply.started": "2025-09-30T13:07:10.192253Z",
     "to_execute": "2025-09-30T13:07:10.171Z"
    },
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:07:10.332\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And adjusts the speed to 0.7.\n",
      "</prompt><role>ASSISTANT</role>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:07:17.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[ 0.0004,  0.0006,  0.0008,  ..., -0.0003, -0.0005, -0.0004]]],\n",
      "       device='cuda:0'), '<gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "    # time_stretch\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/00004768-00000024.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And adjusts the speed to 0.7.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_edit(messages=messages, use_cot=False, output_wav_path=\"data/output/time_stretch.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af233811-ed22-40b1-981d-1260dc64988d",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2025-09-30T13:09:01.877116Z",
     "shell.execute_reply.started": "2025-09-30T13:08:56.059672Z",
     "to_execute": "2025-09-30T13:08:56.029Z"
    },
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:08:56.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And shifts the pitch by 3 steps.\n",
      "</prompt><role>ASSISTANT</role>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:09:01.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[-3.8837e-04, -2.6293e-04,  1.8646e-04,  ...,  4.6090e-05,\n",
      "           6.5749e-05,  1.7663e-05]]], device='cuda:0'), '<gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "    # pitch_shift\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/00004768-00000024.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And shifts the pitch by 3 steps.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = model.speech_edit(messages=messages, use_cot=False, output_wav_path=\"data/output/pitch_shift.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6808b6e2-72ea-458d-b76e-6b4fdb7fd8b5",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2025-09-30T13:10:14.949269Z",
     "shell.execute_reply.started": "2025-09-30T13:10:09.123805Z",
     "to_execute": "2025-09-30T13:10:09.105Z"
    },
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:10:09.258\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And adjusts the volume to 0.6.\n",
      "</prompt><role>ASSISTANT</role>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:10:14.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[-9.3645e-04, -9.3372e-04, -6.7022e-04,  ...,  2.7338e-05,\n",
      "           2.5592e-05, -3.1757e-06]]], device='cuda:0'), '<gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # vol\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/00004768-00000024.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And adjusts the volume to 0.6.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = model.speech_edit(messages=messages, use_cot=False, output_wav_path=\"data/output/vol.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fffeeb20-f456-492a-baee-45ab515614e7",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2025-09-30T13:13:11.906804Z",
     "shell.execute_reply.started": "2025-09-30T13:13:06.101381Z",
     "to_execute": "2025-09-30T13:13:06.098Z"
    },
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:13:06.239\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And add rain to audio.\n",
      "</prompt><role>ASSISTANT</role>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:13:11.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[-0.0146,  0.0316,  0.0304,  ..., -0.0022, -0.0320, -0.0388]]],\n",
      "       device='cuda:0'), '<gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # add sound\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/00004768-00000024.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And add rain to audio.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = model.speech_edit(messages=messages, use_cot=False, output_wav_path=\"data/output/add_sound.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d38c1b9-d6de-47b5-91b7-59753af52ece",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2025-09-30T13:11:41.341147Z",
     "shell.execute_reply.started": "2025-09-30T13:11:39.308476Z",
     "to_execute": "2025-09-30T13:11:39.284Z"
    },
    "isLargeOutputDisplay": false,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:11:39.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_understanding\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1minput: <role>HUMAN</role>Please recognize the language of this speech and transcribe it. Format: oral. This audio may contains the following words or phrases:Project Almanac,Dean Israelite,Jonny Weston,Sofia Black D' Elia,Sam Lerner,Jessie,Quinn,C G I,reverse engineer,timeline<audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><role>ASSISTANT</role>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:11:41.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mGenerated Response: English\tdean israelite said in an interview you wanted it to feel like a youtube video gone wrong mission accomplished the shaky cam during the meltdown stressful but genius\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Context ASR\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral. This audio may contains the following words or phrases:Project Almanac,Dean Israelite,Jonny Weston,Sofia Black D' Elia,Sam Lerner,Jessie,Quinn,C G I,reverse engineer,timeline\",\n",
    "                },\n",
    "                \n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/11302-4_1712960-1908016.wav\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_understanding(messages=messages)\n",
    "    logger.info(f\"Generated Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6818341-6680-4ada-8927-0c669648ec77",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2025-09-30T13:21:47.386263Z",
     "shell.execute_reply.started": "2025-09-30T13:21:42.078520Z",
     "to_execute": "2025-09-30T13:21:42.085Z"
    },
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:21:42.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And change the emotion to happy mood.\n",
      "</prompt><role>ASSISTANT</role>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:21:47.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[0.0021, 0.0027, 0.0026,  ..., 0.0001, 0.0002, 0.0002]]],\n",
      "       device='cuda:0'), '<gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # emotion\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/emotion_00004753-00000079.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And change the emotion to happy mood.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = model.speech_edit(messages=messages, use_cot=False, output_wav_path=\"data/output/emotion.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75e9a653-a479-4194-820d-254939bdcbb7",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2025-09-30T13:24:08.597813Z",
     "shell.execute_reply.started": "2025-09-30T13:24:02.777199Z",
     "to_execute": "2025-09-30T13:24:02.800Z"
    },
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:24:02.801\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And change the accent of the speech to Chengdu.\n",
      "</prompt><role>ASSISTANT</role>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:24:08.595\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[ 0.0005,  0.0007,  0.0010,  ..., -0.0003, -0.0002, -0.0004]]],\n",
      "       device='cuda:0'), '<gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # dialect conversion\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/emotion_00004753-00000079.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And change the accent of the speech to Chengdu.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = model.speech_edit(messages=messages, use_cot=False, output_wav_path=\"data/output/dialect_conversion.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
