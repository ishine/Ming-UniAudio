{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4770bf52-16aa-486e-9a5e-dad64a8ad658",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2025-10-27T07:11:40.070812Z",
     "shell.execute_reply.started": "2025-10-27T07:11:40.067295Z",
     "to_execute": "2025-10-27T07:11:40.179Z"
    },
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Ming-Lite-UniAudio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /path/to/workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28108cc-d941-4910-ac00-6106e68c2d99",
   "metadata": {
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00bfc4a-38b2-4afa-899b-1ea0c6483c7b",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-27 15:05:24,009] [INFO] [add_hooks.py:71:__init__] If you are working on AIStudio, please set 'ANTMONITOR_TFEVENT_PATH' env.\n",
      "2025-10-27 15:05:24,930 - datasets - INFO - PyTorch version 2.6.0 available.\n",
      "BailingMMNativeForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce4754e6cda4c57bbb5931216f34c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at inclusionAI/Ming-UniAudio-16B-A3B were not used when initializing BailingMMNativeForConditionalGeneration: ['audio.decoder.semantic_model.conv1.bias', 'audio.decoder.semantic_model.conv1.weight', 'audio.decoder.semantic_model.conv2.bias', 'audio.decoder.semantic_model.conv2.weight', 'audio.decoder.semantic_model.positional_embedding']\n",
      "- This IS expected if you are initializing BailingMMNativeForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BailingMMNativeForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "BailingMMNativeForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "BailingMMNativeForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Fusing experts: 100%|██████████| 28/28 [01:12<00:00,  2.59s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import AutoProcessor\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import display\n",
    "import ipynbname\n",
    "notebook_path = ipynbname.path()\n",
    "current_dir = os.path.dirname(notebook_path)\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from modeling_bailingmm import BailingMMNativeForConditionalGeneration\n",
    "import random\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "from sentence_manager.sentence_manager import SentenceNormalizer\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "def seed_everything(seed=1895):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class MingAudio:\n",
    "    def __init__(self, model_path, lora_path=None, device=\"cuda:0\", use_grouped_gemm=True):\n",
    "        self.device = device\n",
    "        self.model = BailingMMNativeForConditionalGeneration.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            low_cpu_mem_usage=True,\n",
    "        ).to(self.device)\n",
    "\n",
    "        if use_grouped_gemm and not self.model.config.llm_config.use_grouped_gemm:\n",
    "            self.model.model.fuse_experts()\n",
    "\n",
    "        if lora_path is not None:\n",
    "            self.model = PeftModel.from_pretrained(self.model, lora_path)\n",
    "        self.model = self.model.eval().to(torch.bfloat16).to(self.device)\n",
    "        self.processor = AutoProcessor.from_pretrained(\".\", trust_remote_code=True)\n",
    "        self.tokenizer = self.processor.tokenizer\n",
    "        self.sample_rate = self.processor.audio_processor.sample_rate\n",
    "        self.patch_size = self.processor.audio_processor.patch_size\n",
    "        self.normalizer = self.init_tn_normalizer(tokenizer=self.tokenizer)\n",
    "\n",
    "    def init_tn_normalizer(self, config_file_path=None, tokenizer=None):\n",
    "\n",
    "        if config_file_path is None:\n",
    "            default_config_path = \"sentence_manager/default_config.yaml\"\n",
    "            config_file_path = default_config_path\n",
    "        with open(config_file_path, 'r') as f:\n",
    "            self.sentence_manager_config = yaml.safe_load(f)\n",
    "        if \"split_token\" not in self.sentence_manager_config:\n",
    "            self.sentence_manager_config[\"split_token\"] = []\n",
    "        assert isinstance(self.sentence_manager_config[\"split_token\"], list)\n",
    "        if tokenizer is not None:\n",
    "            self.sentence_manager_config[\"split_token\"].append(re.escape(tokenizer.eos_token))\n",
    "        normalizer = SentenceNormalizer(self.sentence_manager_config.get(\"text_norm\", {}))\n",
    "        \n",
    "        return normalizer\n",
    "\n",
    "    def speech_understanding(self, messages, lang=None):\n",
    "        text = self.processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        image_inputs, video_inputs, audio_inputs = self.processor.process_vision_info(messages)\n",
    "\n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            audios=audio_inputs,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.device)\n",
    "        \n",
    "        if lang is not None:\n",
    "            language = torch.tensor([self.tokenizer.encode(f'{lang}\\t')]).to(inputs['input_ids'].device)\n",
    "            inputs['input_ids'] = torch.cat([inputs['input_ids'], language], dim=1)\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            inputs['attention_mask'] = torch.ones(inputs['input_ids'].shape, dtype=attention_mask.dtype)\n",
    "        for k in inputs.keys():\n",
    "            if k == \"pixel_values\" or k == \"pixel_values_videos\" or k == \"audio_feats\":\n",
    "                inputs[k] = inputs[k].to(dtype=torch.bfloat16)\n",
    "        logger.info(f\"input: {self.tokenizer.decode(inputs['input_ids'].cpu().numpy().tolist()[0])}\")\n",
    "\n",
    "        generated_ids = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            eos_token_id=self.processor.gen_terminator,\n",
    "        )\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        output_text = self.processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )[0]\n",
    "\n",
    "        return output_text\n",
    "\n",
    "    def speech_generation(\n",
    "        self, \n",
    "        text,\n",
    "        prompt_wav_path,\n",
    "        prompt_text,\n",
    "        lang='zh',\n",
    "        output_wav_path='out.wav'\n",
    "    ):\n",
    "        text = self.normalizer.normalize(text)\n",
    "        waveform = self.model.generate_tts(\n",
    "            text=text,\n",
    "            prompt_wav_path=prompt_wav_path,\n",
    "            prompt_text=prompt_text,\n",
    "            patch_size=self.patch_size,\n",
    "            tokenizer=self.tokenizer,\n",
    "            lang=lang,\n",
    "            output_wav_path=output_wav_path,\n",
    "            sample_rate=self.sample_rate,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        return waveform\n",
    "\n",
    "    def speech_edit(\n",
    "        self, \n",
    "        messages,\n",
    "        output_wav_path='out.wav',\n",
    "        use_cot=True\n",
    "    ):\n",
    "        text = self.processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        image_inputs, video_inputs, audio_inputs = self.processor.process_vision_info(messages)\n",
    "\n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            audios=audio_inputs,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.device)\n",
    "\n",
    "        if use_cot:\n",
    "            ans = torch.tensor([self.tokenizer.encode('<answer>')]).to(inputs['input_ids'].device)\n",
    "            inputs['input_ids'] = torch.cat([inputs['input_ids'], ans], dim=1)\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            inputs['attention_mask'] = torch.ones(inputs['input_ids'].shape, dtype=attention_mask.dtype)\n",
    "        for k in inputs.keys():\n",
    "            if k == \"pixel_values\" or k == \"pixel_values_videos\" or k == \"audio_feats\":\n",
    "                inputs[k] = inputs[k].to(dtype=torch.bfloat16)\n",
    "        logger.info(f\"input: {self.tokenizer.decode(inputs['input_ids'].cpu().numpy().tolist()[0])}\")\n",
    "\n",
    "        edited_speech, edited_text = self.model.generate_edit(\n",
    "            **inputs,\n",
    "            tokenizer=self.tokenizer,\n",
    "            output_wav_path=output_wav_path\n",
    "        )\n",
    "        return edited_speech, edited_text\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # model = MingAudio(\"/path/to/model\")\n",
    "    # load base model\n",
    "    model = MingAudio(\"inclusionAI/Ming-UniAudio-16B-A3B\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8c3e1e-690a-4232-94b6-3d0ce26879fa",
   "metadata": {
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170c71f9-336a-433a-9e4a-a6980b938e4d",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 20:54:26.673\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_understanding\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1minput: <role>HUMAN</role>Please recognize the language of this speech and transcribe it. Format: oral.<audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><role>ASSISTANT</role>\u001b[0m\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "2025-09-30 20:54:27,589 - modeling_bailing_moe - WARNING - The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "\u001b[32m2025-09-30 20:54:28.285\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mGenerated Response: Chinese\t现在是不是也该长点心了吧\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # ASR\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral.\",\n",
    "                },\n",
    "                \n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/BAC009S0915W0292.wav\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_understanding(messages=messages)\n",
    "    logger.info(f\"Generated Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de9ed2d-0eb5-4720-b9bc-4e5555ff8827",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": false,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-01 21:47:27.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_understanding\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1minput: <role>HUMAN</role>Please recognize the language of this speech and transcribe it. Format: oral.<audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><role>ASSISTANT</role>川渝\t\u001b[0m\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "2025-10-01 21:47:28,317 - modeling_bailing_moe - WARNING - The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "\u001b[32m2025-10-01 21:47:28.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mGenerated Response: 我难受得很别个都睡了\u001b[0m\n",
      "\u001b[32m2025-10-01 21:47:28.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_understanding\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1minput: <role>HUMAN</role>Please recognize the language of this speech and transcribe it. Format: oral.<audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><role>ASSISTANT</role>湖南\t\u001b[0m\n",
      "\u001b[32m2025-10-01 21:47:29.605\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mGenerated Response: 我们到你屋里大概一点半左右\u001b[0m\n",
      "\u001b[32m2025-10-01 21:47:29.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_understanding\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1minput: <role>HUMAN</role>Please recognize the language of this speech and transcribe it. Format: oral.<audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><role>ASSISTANT</role>闽南\t\u001b[0m\n",
      "\u001b[32m2025-10-01 21:47:30.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mGenerated Response: 宝贝较早休困晚安\u001b[0m\n",
      "\u001b[32m2025-10-01 21:47:30.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_understanding\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1minput: <role>HUMAN</role>Please recognize the language of this speech and transcribe it. Format: oral.<audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><role>ASSISTANT</role>上海\t\u001b[0m\n",
      "\u001b[32m2025-10-01 21:47:30.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mGenerated Response: 阿拉考试还没定下来唻\u001b[0m\n",
      "\u001b[32m2025-10-01 21:47:30.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_understanding\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1minput: <role>HUMAN</role>Please recognize the language of this speech and transcribe it. Format: oral.<audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><role>ASSISTANT</role>Canton\t\u001b[0m\n",
      "\u001b[32m2025-10-01 21:47:31.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mGenerated Response: 你做乜嘢啊系咪唔想倾偈啊\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Dialect ASR\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral.\",\n",
    "                },\n",
    "                \n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/chuanyu_demo.wav\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_understanding(messages=messages, lang=\"川渝\")\n",
    "    logger.info(f\"Generated Response: {response}\")\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral.\",\n",
    "                },\n",
    "                \n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/hunan_demo.wav\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_understanding(messages=messages, lang=\"湖南\")\n",
    "    logger.info(f\"Generated Response: {response}\")\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral.\",\n",
    "                },\n",
    "                \n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/minnan_demo.wav\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_understanding(messages=messages, lang=\"闽南\")\n",
    "    logger.info(f\"Generated Response: {response}\")\n",
    "\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral.\",\n",
    "                },\n",
    "                \n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/shanghai_demo.wav\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_understanding(messages=messages, lang=\"上海\")\n",
    "    logger.info(f\"Generated Response: {response}\")\n",
    "\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral.\",\n",
    "                },\n",
    "                \n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/yueyu_demo.wav\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_understanding(messages=messages, lang=\"Canton\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cc7088-a1ec-4e17-84d3-6047a30a6557",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating zh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<role>HUMAN</role>Please translate the text to speech.\n",
      "在此奉劝大家别乱打美白针。我们的愿景是构建未来服务业的数字化基础设施，为世界带来更多微小而美好的改变。<role>ASSISTANT</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 96/300 [00:13<00:27,  7.37it/s]\n",
      "\u001b[32m2025-09-30 20:56:01.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mwaveform: tensor([[ 5.7267e-04,  1.2557e-03,  8.2468e-04,  ..., -2.2724e-06,\n",
      "         -1.6844e-06, -2.2619e-05]])\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StopInfo: 96 299\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # TTS\n",
    "    waveform = model.speech_generation(\n",
    "        text='我们的愿景是构建未来服务业的数字化基础设施，为世界带来更多微小而美好的改变。',\n",
    "        prompt_wav_path='data/wavs/10002287-00000094.wav',\n",
    "        prompt_text='在此奉劝大家别乱打美白针。',\n",
    "        output_wav_path='data/output/tts.wav',\n",
    "    )\n",
    "    logger.info(f\"waveform: {waveform}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dcaf98-133e-4f77-83e6-4b9c7b1eb096",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 20:57:03.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And insert '实现' before the character or word at index 3.\n",
      "</prompt><role>ASSISTANT</role><answer>\u001b[0m\n",
      "\u001b[32m2025-09-30 20:57:10.166\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[-0.0006, -0.0005, -0.0001,  ...,  0.0016,  0.0013,  0.0012]]],\n",
      "       device='cuda:0'), '<cot_text>有望<edit><framePatch></edit>盘活超万亿的国企存量资产</cot_text><gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "    # load sft model\n",
    "    model = MingAudio(\"inclusionAI/Ming-UniAudio-16B-A3B-Edit\")\n",
    "    # Ins\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/00004768-00000024.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And insert '实现' before the character or word at index 3.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_edit(messages=messages, output_wav_path=\"data/output/ins.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2813319d-6664-4acd-b5b2-62dedeb905c0",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:01:13.133\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And delete the characters or words from index 5 to index 8.\n",
      "</prompt><role>ASSISTANT</role><answer>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:01:17.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[-0.0020, -0.0023, -0.0018,  ..., -0.0025, -0.0023, -0.0020]]],\n",
      "       device='cuda:0'), '<cot_text>有望盘活<edit></edit>国企存量资产</cot_text><gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Del\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/00004768-00000024.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And delete the characters or words from index 5 to index 8.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_edit(messages=messages, output_wav_path=\"data/output/del.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81350375-59fd-47e6-a145-bd8abcc397c8",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:02:49.760\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And substitute '盘活' with '创造'.\n",
      "</prompt><role>ASSISTANT</role><answer>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:02:55.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[-0.0007, -0.0006, -0.0002,  ...,  0.0024,  0.0020,  0.0019]]],\n",
      "       device='cuda:0'), '<cot_text>有望<edit><framePatch></edit>超万亿的国企存量资产</cot_text><gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Sub\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/00004768-00000024.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And substitute '盘活' with '创造'.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_edit(messages=messages, output_wav_path=\"data/output/sub.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e50ebb-6600-46fe-8135-1bb58b202c81",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:05:08.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And denoise the audio.\n",
      "</prompt><role>ASSISTANT</role>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:05:22.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[-8.8062e-03,  1.3475e-02,  5.8888e-03,  ...,  2.6246e-06,\n",
      "          -2.5213e-06,  4.4239e-07]]], device='cuda:0'), '<gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Denoise\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/noreverb_fileid_0.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And denoise the audio.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_edit(messages=messages, use_cot=False, output_wav_path=\"data/output/denoise.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed9e33c-55ea-48cc-b03e-2ab83ed28225",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:07:10.332\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And adjusts the speed to 0.7.\n",
      "</prompt><role>ASSISTANT</role>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:07:17.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[ 0.0004,  0.0006,  0.0008,  ..., -0.0003, -0.0005, -0.0004]]],\n",
      "       device='cuda:0'), '<gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "    # time_stretch\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/00004768-00000024.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And adjusts the speed to 0.7.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_edit(messages=messages, use_cot=False, output_wav_path=\"data/output/time_stretch.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af233811-ed22-40b1-981d-1260dc64988d",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:08:56.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And shifts the pitch by 3 steps.\n",
      "</prompt><role>ASSISTANT</role>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:09:01.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[-3.8837e-04, -2.6293e-04,  1.8646e-04,  ...,  4.6090e-05,\n",
      "           6.5749e-05,  1.7663e-05]]], device='cuda:0'), '<gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "    # pitch_shift\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/00004768-00000024.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And shifts the pitch by 3 steps.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = model.speech_edit(messages=messages, use_cot=False, output_wav_path=\"data/output/pitch_shift.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6808b6e2-72ea-458d-b76e-6b4fdb7fd8b5",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:10:09.258\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And adjusts the volume to 0.6.\n",
      "</prompt><role>ASSISTANT</role>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:10:14.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[-9.3645e-04, -9.3372e-04, -6.7022e-04,  ...,  2.7338e-05,\n",
      "           2.5592e-05, -3.1757e-06]]], device='cuda:0'), '<gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # vol\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/00004768-00000024.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And adjusts the volume to 0.6.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = model.speech_edit(messages=messages, use_cot=False, output_wav_path=\"data/output/vol.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffeeb20-f456-492a-baee-45ab515614e7",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:13:06.239\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And add rain to audio.\n",
      "</prompt><role>ASSISTANT</role>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:13:11.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[-0.0146,  0.0316,  0.0304,  ..., -0.0022, -0.0320, -0.0388]]],\n",
      "       device='cuda:0'), '<gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # add sound\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/00004768-00000024.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And add rain to audio.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = model.speech_edit(messages=messages, use_cot=False, output_wav_path=\"data/output/add_sound.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d38c1b9-d6de-47b5-91b7-59753af52ece",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": false,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:11:39.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_understanding\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1minput: <role>HUMAN</role>Please recognize the language of this speech and transcribe it. Format: oral. This audio may contains the following words or phrases:Project Almanac,Dean Israelite,Jonny Weston,Sofia Black D' Elia,Sam Lerner,Jessie,Quinn,C G I,reverse engineer,timeline<audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><role>ASSISTANT</role>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:11:41.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mGenerated Response: English\tdean israelite said in an interview you wanted it to feel like a youtube video gone wrong mission accomplished the shaky cam during the meltdown stressful but genius\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Context ASR\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral. This audio may contains the following words or phrases:Project Almanac,Dean Israelite,Jonny Weston,Sofia Black D' Elia,Sam Lerner,Jessie,Quinn,C G I,reverse engineer,timeline\",\n",
    "                },\n",
    "                \n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/11302-4_1712960-1908016.wav\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_understanding(messages=messages)\n",
    "    logger.info(f\"Generated Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6818341-6680-4ada-8927-0c669648ec77",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:21:42.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And change the emotion to happy mood.\n",
      "</prompt><role>ASSISTANT</role>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:21:47.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[0.0021, 0.0027, 0.0026,  ..., 0.0001, 0.0002, 0.0002]]],\n",
      "       device='cuda:0'), '<gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # emotion\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/emotion_00004753-00000079.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And change the emotion to happy mood.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = model.speech_edit(messages=messages, use_cot=False, output_wav_path=\"data/output/emotion.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e9a653-a479-4194-820d-254939bdcbb7",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:24:02.801\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And change the accent of the speech to Chengdu.\n",
      "</prompt><role>ASSISTANT</role>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:24:08.595\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[ 0.0005,  0.0007,  0.0010,  ..., -0.0003, -0.0002, -0.0004]]],\n",
      "       device='cuda:0'), '<gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # dialect conversion\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/emotion_00004753-00000079.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And change the accent of the speech to Chengdu.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = model.speech_edit(messages=messages, use_cot=False, output_wav_path=\"data/output/dialect_conversion.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
